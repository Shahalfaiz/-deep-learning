{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14925d78",
   "metadata": {},
   "source": [
    "Q1/Load the dataset, PRSA_Data.csv, into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92ea29c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   No  year  month  day  hour  PM2.5  PM10  SO2   NO2     CO    O3  TEMP  \\\n",
      "0   1  2013      3    1     0    9.0   9.0  3.0  17.0  300.0  89.0  -0.5   \n",
      "1   2  2013      3    1     1    4.0   4.0  3.0  16.0  300.0  88.0  -0.7   \n",
      "2   3  2013      3    1     2    7.0   7.0  NaN  17.0  300.0  60.0  -1.2   \n",
      "3   4  2013      3    1     3    3.0   3.0  5.0  18.0    NaN   NaN  -1.4   \n",
      "4   5  2013      3    1     4    3.0   3.0  7.0   NaN  200.0  84.0  -1.9   \n",
      "\n",
      "     PRES  DEWP  RAIN   wd  WSPM station  \n",
      "0  1024.5 -21.4   0.0  NNW   5.7  Dongsi  \n",
      "1  1025.1 -22.1   0.0   NW   3.9  Dongsi  \n",
      "2  1025.3 -24.6   0.0  NNW   5.3  Dongsi  \n",
      "3  1026.2 -25.5   0.0    N   4.9  Dongsi  \n",
      "4  1027.1 -24.5   0.0  NNW   3.2  Dongsi  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"PRSA_Data_Dongsi_20130301-20170228.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605a1bfc",
   "metadata": {},
   "source": [
    "Q2/Clean the data and check missing values for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19f67fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "No            0\n",
      "year          0\n",
      "month         0\n",
      "day           0\n",
      "hour          0\n",
      "PM2.5       750\n",
      "PM10        553\n",
      "SO2         663\n",
      "NO2        1601\n",
      "CO         3197\n",
      "O3          664\n",
      "TEMP         20\n",
      "PRES         20\n",
      "DEWP         20\n",
      "RAIN         20\n",
      "wd           78\n",
      "WSPM         14\n",
      "station       0\n",
      "dtype: int64\n",
      "\n",
      "After cleaning, missing values per column:\n",
      "No         0\n",
      "year       0\n",
      "month      0\n",
      "day        0\n",
      "hour       0\n",
      "PM2.5      0\n",
      "PM10       0\n",
      "SO2        0\n",
      "NO2        0\n",
      "CO         0\n",
      "O3         0\n",
      "TEMP       0\n",
      "PRES       0\n",
      "DEWP       0\n",
      "RAIN       0\n",
      "wd         0\n",
      "WSPM       0\n",
      "station    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values)\n",
    "df_cleaned = df.dropna()\n",
    "print(\"\\nAfter cleaning, missing values per column:\")\n",
    "print(df_cleaned.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390df91b",
   "metadata": {},
   "source": [
    "Q3/Convert all categorical variables to numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9104fd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   No  year  month  day  hour  PM2.5  PM10   SO2   NO2     CO    O3  TEMP  \\\n",
      "0   1  2013      3    1     0    9.0   9.0   3.0  17.0  300.0  89.0  -0.5   \n",
      "1   2  2013      3    1     1    4.0   4.0   3.0  16.0  300.0  88.0  -0.7   \n",
      "5   6  2013      3    1     5    4.0   4.0   9.0  25.0  300.0  78.0  -2.4   \n",
      "6   7  2013      3    1     6    5.0   5.0  10.0  29.0  400.0  67.0  -2.5   \n",
      "7   8  2013      3    1     7    3.0   6.0  12.0  40.0  400.0  52.0  -1.4   \n",
      "\n",
      "     PRES  DEWP  RAIN  WSPM  wd_encoded  station_encoded  \n",
      "0  1024.5 -21.4   0.0   5.7           6                0  \n",
      "1  1025.1 -22.1   0.0   3.9           7                0  \n",
      "5  1027.5 -21.3   0.0   2.4           7                0  \n",
      "6  1028.2 -20.4   0.0   2.2           7                0  \n",
      "7  1029.5 -20.4   0.0   3.0           6                0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "C:\\Users\\aldul\\AppData\\Local\\Temp\\ipykernel_6904\\2577563760.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['wd_encoded'] = label_encoder.fit_transform(df_cleaned['wd'])\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "C:\\Users\\aldul\\AppData\\Local\\Temp\\ipykernel_6904\\2577563760.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['station_encoded'] = label_encoder.fit_transform(df_cleaned['station'])\n",
      "C:\\Users\\aldul\\AppData\\Local\\Temp\\ipykernel_6904\\2577563760.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned.drop(['wd', 'station'], axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df_cleaned['wd_encoded'] = label_encoder.fit_transform(df_cleaned['wd'])\n",
    "df_cleaned['station_encoded'] = label_encoder.fit_transform(df_cleaned['station'])\n",
    "df_cleaned.drop(['wd', 'station'], axis=1, inplace=True)\n",
    "print(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5f246a",
   "metadata": {},
   "source": [
    "Q4/Split the data into 80% of training and 20% of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db1eb47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training features: (24270, 17)\n",
      "Shape of test features: (6068, 17)\n",
      "Shape of training target variable: (24270,)\n",
      "Shape of test target variable: (6068,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df_cleaned.drop(columns=['PM2.5']) \n",
    "y = df_cleaned['PM2.5'] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Shape of training features:\", X_train.shape)\n",
    "print(\"Shape of test features:\", X_test.shape)\n",
    "print(\"Shape of training target variable:\", y_train.shape)\n",
    "print(\"Shape of test target variable:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7926672",
   "metadata": {},
   "source": [
    "Q5/Preprocess the data using the normalization method to convert all features into the range of [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b30c59af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Training Features:\n",
      "         No  year     month       day      hour      PM10    SO2       NO2  \\\n",
      "0  0.717252  0.75  0.000000  0.366667  0.913043  0.037775  0.045  0.207031   \n",
      "1  0.836238  0.75  0.545455  0.100000  0.739130  0.117524  0.025  0.082031   \n",
      "2  0.040784  0.00  0.272727  0.933333  0.608696  0.069255  0.010  0.027344   \n",
      "3  0.524399  0.50  0.272727  0.166667  0.130435  0.038825  0.005  0.035156   \n",
      "4  0.370904  0.25  0.636364  0.766667  0.913043  0.326338  0.015  0.195312   \n",
      "\n",
      "         CO        O3      TEMP      PRES      DEWP  RAIN      WSPM  \\\n",
      "0  0.080808  0.014019  0.220280  0.683919  0.210608   0.0  0.152381   \n",
      "1  0.141414  0.210280  0.842657  0.268022  0.853354   0.0  0.219048   \n",
      "2  0.010101  0.121495  0.791958  0.316081  0.433697   0.0  0.485714   \n",
      "3  0.010101  0.066355  0.382867  0.678373  0.377535   0.0  0.180952   \n",
      "4  0.060606  0.087850  0.737762  0.332717  0.854914   0.0  0.200000   \n",
      "\n",
      "   wd_encoded  station_encoded  \n",
      "0    0.266667              0.0  \n",
      "1    0.600000              0.0  \n",
      "2    0.466667              0.0  \n",
      "3    0.266667              0.0  \n",
      "4    0.000000              0.0  \n",
      "\n",
      "Normalized Test Features:\n",
      "         No  year     month       day      hour      PM10    SO2       NO2  \\\n",
      "0  0.777401  0.75  0.272727  0.266667  0.782609  0.412382  0.035  0.125000   \n",
      "1  0.135613  0.00  0.727273  0.466667  0.130435  0.009444  0.000  0.183594   \n",
      "2  0.432935  0.25  0.909091  0.733333  0.521739  0.119622  0.175  0.277344   \n",
      "3  0.746314  0.75  0.090909  0.766667  0.347826  0.043022  0.060  0.269531   \n",
      "4  0.328409  0.25  0.454545  0.733333  0.826087  0.037775  0.005  0.109375   \n",
      "\n",
      "         CO        O3      TEMP      PRES      DEWP  RAIN      WSPM  \\\n",
      "0  0.040404  0.095327  0.569930  0.329020  0.516381   0.0  0.295238   \n",
      "1  0.030303  0.010280  0.615385  0.408503  0.638066   0.0  0.190476   \n",
      "2  0.191919  0.001869  0.398601  0.634011  0.567863   0.0  0.057143   \n",
      "3  0.131313  0.002804  0.201049  0.835490  0.274571   0.0  0.142857   \n",
      "4  0.040404  0.169159  0.776224  0.249538  0.744150   0.0  0.066667   \n",
      "\n",
      "   wd_encoded  station_encoded  \n",
      "0    0.600000              0.0  \n",
      "1    0.466667              0.0  \n",
      "2    0.066667              0.0  \n",
      "3    0.066667              0.0  \n",
      "4    0.533333              0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "X_train_normalized = pd.DataFrame(X_train_normalized, columns=X_train.columns)\n",
    "X_test_normalized = pd.DataFrame(X_test_normalized, columns=X_test.columns)\n",
    "print(\"Normalized Training Features:\")\n",
    "print(X_train_normalized.head())\n",
    "print(\"\\nNormalized Test Features:\")\n",
    "print(X_test_normalized.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1767a9",
   "metadata": {},
   "source": [
    "Q6/Build a neuron network with two hidden layers of 20 and 10 neurons to forecast PM2.5 using all other features and TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5dab92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\aldul\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 12075.5176 - val_loss: 5996.6470\n",
      "Epoch 2/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5098.9580 - val_loss: 2684.6990\n",
      "Epoch 3/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2205.2607 - val_loss: 1588.6648\n",
      "Epoch 4/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1475.4918 - val_loss: 1320.0905\n",
      "Epoch 5/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1265.4447 - val_loss: 1179.7118\n",
      "Epoch 6/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1158.5916 - val_loss: 1102.1050\n",
      "Epoch 7/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1081.1127 - val_loss: 1038.1470\n",
      "Epoch 8/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1010.2197 - val_loss: 1012.9169\n",
      "Epoch 9/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 979.3367 - val_loss: 980.0403\n",
      "Epoch 10/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 954.2153 - val_loss: 962.6556\n",
      "Epoch 11/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 950.0699 - val_loss: 953.4103\n",
      "Epoch 12/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 917.4760 - val_loss: 944.4005\n",
      "Epoch 13/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 869.5811 - val_loss: 939.0188\n",
      "Epoch 14/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 884.4963 - val_loss: 933.8595\n",
      "Epoch 15/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 864.2377 - val_loss: 930.1851\n",
      "Epoch 16/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 905.4529 - val_loss: 930.6417\n",
      "Epoch 17/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 917.8000 - val_loss: 926.3463\n",
      "Epoch 18/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 863.6762 - val_loss: 921.9401\n",
      "Epoch 19/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 827.1874 - val_loss: 917.5986\n",
      "Epoch 20/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 882.3929 - val_loss: 914.8607\n",
      "Epoch 21/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 895.2001 - val_loss: 913.6663\n",
      "Epoch 22/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 829.4990 - val_loss: 913.7482\n",
      "Epoch 23/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 848.4539 - val_loss: 907.7219\n",
      "Epoch 24/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 854.7877 - val_loss: 906.0921\n",
      "Epoch 25/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 894.6907 - val_loss: 914.6914\n",
      "Epoch 26/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 846.8560 - val_loss: 901.1164\n",
      "Epoch 27/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 876.4932 - val_loss: 899.5517\n",
      "Epoch 28/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 892.2524 - val_loss: 907.0526\n",
      "Epoch 29/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 842.4199 - val_loss: 899.2306\n",
      "Epoch 30/30\n",
      "\u001b[1m607/607\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 895.8296 - val_loss: 897.0139\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 730.9909\n",
      "Test Loss: 776.17138671875\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "num_features = X_train_normalized.shape[1]\n",
    "model = Sequential([\n",
    "    Dense(20, activation='relu', input_shape=(num_features,)),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(1)  \n",
    "])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "history = model.fit(X_train_normalized, y_train, epochs=30, batch_size=32, validation_split=0.2)\n",
    "loss = model.evaluate(X_test_normalized, y_test)\n",
    "print(\"Test Loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4130282",
   "metadata": {},
   "source": [
    "Does it overfit or underfit the data? Please justify your answer.\n",
    "the model fits the data nicely without trying too hard to remember everything from the training data (overfitting) or not trying hard enough (underfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de7eadc",
   "metadata": {},
   "source": [
    "Q7/Tune the model using the following hyperparameters using keras-tuner:\n",
    "First hidden layer with units between 20 and 50 with a step size of 5\n",
    "Second hidden layer with units between 5 and 10 with a step size of 1\n",
    "The dropout rate for both hidden layer is between 0.2 and 0.8 with a step size of 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f7374a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aldul\\AppData\\Local\\Temp\\ipykernel_6904\\1468123144.py:4: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  import kerastuner as kt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if ( 'keras-tuner' not in sys.modules):\n",
    "    !pip install -q -U keras-tuner\n",
    "    import kerastuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f6bfab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir\\my_project\\tuner0.json\n",
      "Optimal Hyperparameters:\n",
      "First Hidden Layer Units: 40\n",
      "First Hidden Layer Dropout Rate: 0.2\n",
      "Second Hidden Layer Units: 6\n",
      "Second Hidden Layer Dropout Rate: 0.5\n"
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(units=hp.Int('units_1', min_value=20, max_value=50, step=5),\n",
    "                           activation='relu', input_shape=(17,)))\n",
    "    model.add(layers.Dropout(rate=hp.Float('dropout_1', min_value=0.2, max_value=0.8, step=0.1)))\n",
    "    model.add(layers.Dense(units=hp.Int('units_2', min_value=5, max_value=10, step=1), activation='relu'))\n",
    "    model.add(layers.Dropout(rate=hp.Float('dropout_2', min_value=0.2, max_value=0.8, step=0.1)))\n",
    "    model.add(layers.Dense(1))  \n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "    return model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=5,\n",
    "    directory='my_dir', \n",
    "    project_name='my_project'  \n",
    ")\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Optimal Hyperparameters:\")\n",
    "print(f\"First Hidden Layer Units: {best_hp.get('units_1')}\")\n",
    "print(f\"First Hidden Layer Dropout Rate: {best_hp.get('dropout_1')}\")\n",
    "print(f\"Second Hidden Layer Units: {best_hp.get('units_2')}\")\n",
    "print(f\"Second Hidden Layer Dropout Rate: {best_hp.get('dropout_2')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36248e45",
   "metadata": {},
   "source": [
    "Q8/Build a neuron network with two hidden layers of 20 and 10 neurons to forecast PM2.5 using all other features and PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "791eaa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 14508.6416\n",
      "Epoch [20/100], Loss: 14495.9121\n",
      "Epoch [30/100], Loss: 14482.4395\n",
      "Epoch [40/100], Loss: 14466.2432\n",
      "Epoch [50/100], Loss: 14444.9014\n",
      "Epoch [60/100], Loss: 14416.0254\n",
      "Epoch [70/100], Loss: 14377.2256\n",
      "Epoch [80/100], Loss: 14325.8477\n",
      "Epoch [90/100], Loss: 14258.8281\n",
      "Epoch [100/100], Loss: 14172.7500\n",
      "Test Loss: 14047.3486\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "X_train_tensor = torch.tensor(X_train_normalized, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test_normalized, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "input_size = X_train.shape[1]\n",
    "model = NeuralNetwork(input_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    outputs = model(X_test_tensor)\n",
    "    test_loss = criterion(outputs, y_test_tensor)\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b893aae5",
   "metadata": {},
   "source": [
    "Does it overfit or underfit the data? Please justify your answer.\n",
    "It appears that the model is overfitting the data. the model's loss on the training data keeps decreasing over time, but the loss on the test data doesn't decrease as much and remains consistently higher. This indicates that the model is fitting too closely to the training data, leading to poorer performance on new data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
