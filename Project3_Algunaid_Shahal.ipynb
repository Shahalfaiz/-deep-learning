{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "652deb6f",
   "metadata": {},
   "source": [
    "Q1/Load the dataset, PRSA_Data.csv, into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79009c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   No  year  month  day  hour  PM2.5  PM10   SO2   NO2     CO    O3  TEMP  \\\n",
      "0   1  2013      3    1     0    4.0   4.0   4.0   7.0  300.0  77.0  -0.7   \n",
      "1   2  2013      3    1     1    8.0   8.0   4.0   7.0  300.0  77.0  -1.1   \n",
      "2   3  2013      3    1     2    7.0   7.0   5.0  10.0  300.0  73.0  -1.1   \n",
      "3   4  2013      3    1     3    6.0   6.0  11.0  11.0  300.0  72.0  -1.4   \n",
      "4   5  2013      3    1     4    3.0   3.0  12.0  12.0  300.0  72.0  -2.0   \n",
      "\n",
      "     PRES  DEWP  RAIN   wd  WSPM       station  \n",
      "0  1023.0 -18.8   0.0  NNW   4.4  Aotizhongxin  \n",
      "1  1023.2 -18.2   0.0    N   4.7  Aotizhongxin  \n",
      "2  1023.5 -18.2   0.0  NNW   5.6  Aotizhongxin  \n",
      "3  1024.5 -19.4   0.0   NW   3.1  Aotizhongxin  \n",
      "4  1025.2 -19.5   0.0    N   2.0  Aotizhongxin  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"PRSA_Data_Aotizhongxin_20130301-20170228.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3a348c",
   "metadata": {},
   "source": [
    "Q2/Clean the data and check missing values for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27762f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "No            0\n",
      "year          0\n",
      "month         0\n",
      "day           0\n",
      "hour          0\n",
      "PM2.5       925\n",
      "PM10        718\n",
      "SO2         935\n",
      "NO2        1023\n",
      "CO         1776\n",
      "O3         1719\n",
      "TEMP         20\n",
      "PRES         20\n",
      "DEWP         20\n",
      "RAIN         20\n",
      "wd           81\n",
      "WSPM         14\n",
      "station       0\n",
      "dtype: int64\n",
      "\n",
      "After cleaning, missing values per column:\n",
      "No         0\n",
      "year       0\n",
      "month      0\n",
      "day        0\n",
      "hour       0\n",
      "PM2.5      0\n",
      "PM10       0\n",
      "SO2        0\n",
      "NO2        0\n",
      "CO         0\n",
      "O3         0\n",
      "TEMP       0\n",
      "PRES       0\n",
      "DEWP       0\n",
      "RAIN       0\n",
      "wd         0\n",
      "WSPM       0\n",
      "station    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values)\n",
    "df_cleaned = df.dropna()\n",
    "print(\"\\nAfter cleaning, missing values per column:\")\n",
    "print(df_cleaned.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddd6560",
   "metadata": {},
   "source": [
    "Q3/Perform feature engineering on the date feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e417c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_vars = ['wd']  \n",
    "data_encoded = pd.get_dummies(df, columns=categorical_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ef54c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year  month  day  hour  PM2.5  PM10   SO2   NO2     CO    O3  TEMP    PRES  \\\n",
      "0  2013      3    1     0    4.0   4.0   4.0   7.0  300.0  77.0  -0.7  1023.0   \n",
      "1  2013      3    1     1    8.0   8.0   4.0   7.0  300.0  77.0  -1.1  1023.2   \n",
      "2  2013      3    1     2    7.0   7.0   5.0  10.0  300.0  73.0  -1.1  1023.5   \n",
      "3  2013      3    1     3    6.0   6.0  11.0  11.0  300.0  72.0  -1.4  1024.5   \n",
      "4  2013      3    1     4    3.0   3.0  12.0  12.0  300.0  72.0  -2.0  1025.2   \n",
      "\n",
      "   DEWP  RAIN   wd  WSPM       station  day_of_week  hour_of_day  \n",
      "0 -18.8   0.0  NNW   4.4  Aotizhongxin            1            1  \n",
      "1 -18.2   0.0    N   4.7  Aotizhongxin            2            2  \n",
      "2 -18.2   0.0  NNW   5.6  Aotizhongxin            3            3  \n",
      "3 -19.4   0.0   NW   3.1  Aotizhongxin            4            4  \n",
      "4 -19.5   0.0    N   2.0  Aotizhongxin            5            5  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "df['day_of_week'] = df['No'] % 7  \n",
    "df['hour_of_day'] = df['No'] % 24  \n",
    "\n",
    "df.drop(['No'], axis=1, inplace=True)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "344df3ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year             int64\n",
       "month            int64\n",
       "day              int64\n",
       "hour             int64\n",
       "PM2.5          float64\n",
       "PM10           float64\n",
       "SO2            float64\n",
       "NO2            float64\n",
       "CO             float64\n",
       "O3             float64\n",
       "TEMP           float64\n",
       "PRES           float64\n",
       "DEWP           float64\n",
       "RAIN           float64\n",
       "wd              object\n",
       "WSPM           float64\n",
       "station         object\n",
       "day_of_week      int64\n",
       "hour_of_day      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14645b4e",
   "metadata": {},
   "source": [
    "Q4/Split the data into 80% of training and 20% of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17b7f4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (28051, 18) (28051,)\n",
      "Test set shape: (7013, 18) (7013,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(['wd'], axis=1)  \n",
    "y = df['wd']  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341a0845",
   "metadata": {},
   "source": [
    "Q5/Preprocess the data using the normalization method to convert all features into the range of [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b16ae04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Training Set:\n",
      "   year     month       day      hour     PM2.5      PM10       SO2       NO2  \\\n",
      "0  0.25  0.818182  0.466667  0.043478  0.178873  0.193483  0.023966  0.329861   \n",
      "1  0.75  0.454545  0.666667  0.347826  0.143662  0.109980  0.008715  0.107639   \n",
      "2  0.25  0.454545  0.133333  0.434783  0.121127  0.088595  0.130719  0.194444   \n",
      "3  0.00  0.181818  0.833333  0.695652  0.290141  0.285132  0.511983  0.315972   \n",
      "4  0.50  0.363636  0.100000  0.043478  0.026761  0.020367  0.008715  0.076389   \n",
      "\n",
      "         CO        O3      TEMP      PRES      DEWP  RAIN      WSPM  \\\n",
      "0  0.090909  0.004224  0.420593  0.397504  0.660377   0.0  0.000000   \n",
      "1  0.121212  0.124852  0.748691  0.229947  0.900943   0.0  0.116071   \n",
      "2  0.080808  0.205271  0.818499  0.244207  0.838050   0.0  0.089286   \n",
      "3  0.141414  0.101200  0.518325  0.311943  0.556604   0.0  0.276786   \n",
      "4  0.010101  0.183984  0.567190  0.499109  0.405660   0.0  0.196429   \n",
      "\n",
      "   day_of_week  hour_of_day  \n",
      "0     0.500000     0.086957  \n",
      "1     0.000000     0.391304  \n",
      "2     0.166667     0.478261  \n",
      "3     0.166667     0.739130  \n",
      "4     0.666667     0.086957  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "X_train_numeric = X_train.drop(['station'], axis=1)\n",
    "X_test_numeric = X_test.drop(['station'], axis=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_normalized = scaler.fit_transform(X_train_numeric)\n",
    "X_test_normalized = scaler.transform(X_test_numeric)\n",
    "\n",
    "X_train_normalized = pd.DataFrame(X_train_normalized, columns=X_train_numeric.columns)\n",
    "X_test_normalized = pd.DataFrame(X_test_normalized, columns=X_test_numeric.columns)\n",
    "\n",
    "print(\"Normalized Training Set:\")\n",
    "print(X_train_normalized.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445483e4",
   "metadata": {},
   "source": [
    "Q6/Build a neuron network with two hidden layers of 20 and 10 neurons to forecast wd using all other features and TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26aec9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "y_train_one_hot = tf.keras.utils.to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "\n",
    "print(\"Number of classes:\", num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cc55689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1394 - loss: 2.7941 - val_accuracy: 0.1467 - val_loss: 2.7204\n",
      "Epoch 2/10\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1406 - loss: 2.7140 - val_accuracy: 0.1467 - val_loss: 2.6896\n",
      "Epoch 3/10\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1456 - loss: 2.6836 - val_accuracy: 0.1467 - val_loss: 2.6809\n",
      "Epoch 4/10\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1488 - loss: 2.6770 - val_accuracy: 0.1467 - val_loss: 2.6771\n",
      "Epoch 5/10\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1428 - loss: 2.6783 - val_accuracy: 0.1467 - val_loss: 2.6750\n",
      "Epoch 6/10\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.1459 - loss: 2.6710 - val_accuracy: 0.1467 - val_loss: 2.6736\n",
      "Epoch 7/10\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1435 - loss: 2.6780 - val_accuracy: 0.1467 - val_loss: 2.6727\n",
      "Epoch 8/10\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1436 - loss: 2.6731 - val_accuracy: 0.1467 - val_loss: 2.6720\n",
      "Epoch 9/10\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1457 - loss: 2.6760 - val_accuracy: 0.1467 - val_loss: 2.6716\n",
      "Epoch 10/10\n",
      "\u001b[1m702/702\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1462 - loss: 2.6728 - val_accuracy: 0.1467 - val_loss: 2.6713\n",
      "\u001b[1m 60/220\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1511 - loss: 2.6772"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1498 - loss: 2.6685\n",
      "Test Loss: 2.665804862976074\n",
      "Test Accuracy: 0.1487237960100174\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "y_train_one_hot = tf.keras.utils.to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "\n",
    "num_features = X_train_normalized.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(20, activation='relu', input_shape=(num_features,)),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')  \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_normalized, y_train_one_hot, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test_normalized, tf.keras.utils.to_categorical(label_encoder.transform(y_test), num_classes=num_classes))\n",
    "\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafacf02",
   "metadata": {},
   "source": [
    "Does it overfit or underfit the data? Please justify your answer. it is more likely to be underfitting rather than overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af8797e",
   "metadata": {},
   "source": [
    "Q7/Tune the model using the following hyperparameters using keras-tuner:\n",
    "First hidden layer with units between 20 and 100 with a step size of 5\n",
    "Second hidden layer with units between 10 and 50 with a step size of 2\n",
    "The dropout rate for both hidden layer is between 0.2 and 0.8 with a step size of 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1c90d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aldul\\AppData\\Local\\Temp\\ipykernel_7240\\1468123144.py:4: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  import kerastuner as kt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if ( 'keras-tuner' not in sys.modules):\n",
    "    !pip install -q -U keras-tuner\n",
    "    import kerastuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16307caf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 00m 18s]\n",
      "val_accuracy: 0.19446080923080444\n",
      "\n",
      "Best val_accuracy So Far: 0.19446080923080444\n",
      "Total elapsed time: 00h 00m 18s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from kerastuner import HyperModel, RandomSearch\n",
    "from tensorflow import keras\n",
    "from kerastuner import HyperParameters\n",
    "\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['wd'] = label_encoder.fit_transform(df['wd'])\n",
    "\n",
    "X = df.drop(columns=['wd']) \n",
    "y = df['wd']  \n",
    "\n",
    "\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train_imputed)\n",
    "X_test_normalized = scaler.transform(X_test_imputed)\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "\n",
    "class MyHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Dense(units=hp.Int('units_1', min_value=20, max_value=100, step=5),\n",
    "                                     activation='relu', input_shape=self.input_shape))\n",
    "        model.add(keras.layers.Dropout(rate=hp.Float('dropout_1', min_value=0.2, max_value=0.8, step=0.1)))\n",
    "        model.add(keras.layers.Dense(units=hp.Int('units_2', min_value=10, max_value=50, step=2),\n",
    "                                     activation='relu'))\n",
    "        model.add(keras.layers.Dropout(rate=hp.Float('dropout_2', min_value=0.2, max_value=0.8, step=0.1)))\n",
    "        model.add(keras.layers.Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(optimizer=keras.optimizers.Adam(),\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "hypermodel = MyHyperModel(input_shape=X_train_normalized.shape[1:], num_classes=len(np.unique(y_train_encoded)))\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=1,\n",
    "    directory='tuning_directory',\n",
    "    project_name='wind_direction_tuning',\n",
    "    overwrite=True \n",
    ")\n",
    "\n",
    "tuner.search(X_train_normalized, y_train_encoded, epochs=10, validation_split=0.2)\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dce09b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "{'units_1': 40, 'dropout_1': 0.8, 'units_2': 28, 'dropout_2': 0.7}\n",
      "Epoch 1/10\n",
      "\u001b[1m637/637\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1060 - loss: 2.7840 - val_accuracy: 0.1471 - val_loss: 2.6765\n",
      "Epoch 2/10\n",
      "\u001b[1m637/637\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1378 - loss: 2.6716 - val_accuracy: 0.1471 - val_loss: 2.6528\n",
      "Epoch 3/10\n",
      "\u001b[1m637/637\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1391 - loss: 2.6550 - val_accuracy: 0.1495 - val_loss: 2.6149\n",
      "Epoch 4/10\n",
      "\u001b[1m637/637\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1467 - loss: 2.6325 - val_accuracy: 0.1646 - val_loss: 2.5816\n",
      "Epoch 5/10\n",
      "\u001b[1m637/637\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1512 - loss: 2.6046 - val_accuracy: 0.1597 - val_loss: 2.5744\n",
      "Epoch 6/10\n",
      "\u001b[1m637/637\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1530 - loss: 2.5941 - val_accuracy: 0.1701 - val_loss: 2.5492\n",
      "Epoch 7/10\n",
      "\u001b[1m637/637\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1538 - loss: 2.5886 - val_accuracy: 0.1697 - val_loss: 2.5399\n",
      "Epoch 8/10\n",
      "\u001b[1m637/637\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1594 - loss: 2.5695 - val_accuracy: 0.1907 - val_loss: 2.5322\n",
      "Epoch 9/10\n",
      "\u001b[1m637/637\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.1557 - loss: 2.5754 - val_accuracy: 0.1939 - val_loss: 2.5223\n",
      "Epoch 10/10\n",
      "\u001b[1m637/637\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1686 - loss: 2.5571 - val_accuracy: 0.2035 - val_loss: 2.5127\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1974 - loss: 2.5028\n",
      "Test Accuracy: 0.2013201266527176\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_hps.values)\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "best_model.fit(X_train_normalized, y_train_encoded, epochs=10, validation_split=0.2)\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test_normalized, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3350c7f",
   "metadata": {},
   "source": [
    "Q8/Build a neuron network with two hidden layers of 20 and 10 neurons to forecast wd using all other features and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "012563fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 10.7704\n",
      "Epoch [20/50], Loss: 17.6362\n",
      "Epoch [30/50], Loss: 17.8103\n",
      "Epoch [40/50], Loss: 12.9866\n",
      "Epoch [50/50], Loss: 20.8596\n",
      "Test Loss: 17.009336471557617\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_normalized, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_normalized, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_size1 = 20\n",
    "hidden_size2 = 10\n",
    "model = NeuralNet(input_size, hidden_size1, hidden_size2)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) \n",
    "\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets.unsqueeze(1))  \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "with torch.no_grad():\n",
    "    y_pred_tensor = model(X_test_tensor)\n",
    "    test_loss = criterion(y_pred_tensor, y_test_tensor.unsqueeze(1)).item()\n",
    "    print(\"Test Loss:\", test_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44556e7f",
   "metadata": {},
   "source": [
    "Does it overfit or underfit the data? Please justify your answer. it seems that the model is  the model seems to be overfitting slightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dcafe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736c86d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0bd0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bda0baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fa1215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f808233",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
