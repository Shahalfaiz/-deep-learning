{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e7d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4050d34a",
   "metadata": {},
   "source": [
    "Q1/Load the dataset, airfoil_self_noise.DAT, into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0ca8665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0    1       2     3         4        5\n",
      "0   800  0.0  0.3048  71.3  0.002663  126.201\n",
      "1  1000  0.0  0.3048  71.3  0.002663  125.201\n",
      "2  1250  0.0  0.3048  71.3  0.002663  125.951\n",
      "3  1600  0.0  0.3048  71.3  0.002663  127.591\n",
      "4  2000  0.0  0.3048  71.3  0.002663  127.461\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('airfoil_self_noise.DAT', sep='\\t', header=None)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d09f9ad",
   "metadata": {},
   "source": [
    "Q2/Clean the data and check missing values for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5e84bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "5    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daae070",
   "metadata": {},
   "source": [
    "Q3/Split the data into 80% of training and 20% of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93f417f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (1202, 5) (1202,)\n",
      "Test dataset shape : (301, 5) (301,)\n"
     ]
    }
   ],
   "source": [
    "X = df.iloc[:, :-1]  \n",
    "y = df.iloc[:, -1]   \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Training dataset shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test dataset shape :\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaf99f3",
   "metadata": {},
   "source": [
    " Build a simple linear regression to forecast \"Scaled sound pressure level\" using all other features and scikit-learn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e603db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 22.128643318247306\n",
      "R-squared Score: 0.5582979754897279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared Score:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849ca502",
   "metadata": {},
   "source": [
    " What's the test error for this scikit-learn regression model?\n",
    " the test error for this scikit-learn regression model is 22.128643318247306"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e928e5",
   "metadata": {},
   "source": [
    "Q4/Preprocess the data using the normalization method to convert all features into the range of [0,1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d50d223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "D:\\aldul\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e30e2af",
   "metadata": {},
   "source": [
    "Q5/Build a deep learning regression model to forecast \"Scaled sound pressure level\" using all other features and TensorFlow. Please use only two layers of the neuron network.\n",
    "You choose the number of neurons to use in the first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d84fd211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\aldul\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 15595.2451 - val_loss: 15518.6904\n",
      "Epoch 2/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15417.8809 - val_loss: 15330.4395\n",
      "Epoch 3/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15282.2158 - val_loss: 15028.8047\n",
      "Epoch 4/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14901.5391 - val_loss: 14615.1152\n",
      "Epoch 5/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14377.7861 - val_loss: 14100.9980\n",
      "Epoch 6/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13990.1465 - val_loss: 13493.5938\n",
      "Epoch 7/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13228.6484 - val_loss: 12803.1562\n",
      "Epoch 8/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12672.0771 - val_loss: 12039.3633\n",
      "Epoch 9/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11826.9746 - val_loss: 11220.6133\n",
      "Epoch 10/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10896.6523 - val_loss: 10347.6660\n",
      "Epoch 11/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9995.5664 - val_loss: 9455.9502\n",
      "Epoch 12/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9206.6396 - val_loss: 8567.4238\n",
      "Epoch 13/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8271.6279 - val_loss: 7684.2188\n",
      "Epoch 14/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7420.3574 - val_loss: 6832.6904\n",
      "Epoch 15/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6528.7642 - val_loss: 6020.3286\n",
      "Epoch 16/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5613.7690 - val_loss: 5259.3213\n",
      "Epoch 17/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4998.2295 - val_loss: 4563.4814\n",
      "Epoch 18/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4230.1909 - val_loss: 3933.8379\n",
      "Epoch 19/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3743.8293 - val_loss: 3379.6387\n",
      "Epoch 20/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3126.4609 - val_loss: 2899.5286\n",
      "Epoch 21/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2717.7957 - val_loss: 2483.6958\n",
      "Epoch 22/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2359.3718 - val_loss: 2146.1907\n",
      "Epoch 23/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2049.2593 - val_loss: 1862.1122\n",
      "Epoch 24/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1679.0747 - val_loss: 1630.8036\n",
      "Epoch 25/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1586.6915 - val_loss: 1450.2205\n",
      "Epoch 26/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1412.5941 - val_loss: 1316.9176\n",
      "Epoch 27/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1173.9067 - val_loss: 1209.8512\n",
      "Epoch 28/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1148.9485 - val_loss: 1127.6050\n",
      "Epoch 29/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1046.5345 - val_loss: 1063.2468\n",
      "Epoch 30/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 953.5423 - val_loss: 1017.0450\n",
      "Epoch 31/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 986.7947 - val_loss: 982.5791\n",
      "Epoch 32/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 953.5842 - val_loss: 957.6111\n",
      "Epoch 33/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 958.2219 - val_loss: 936.9278\n",
      "Epoch 34/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 939.6537 - val_loss: 919.7560\n",
      "Epoch 35/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 959.1085 - val_loss: 904.4308\n",
      "Epoch 36/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 878.6562 - val_loss: 889.7405\n",
      "Epoch 37/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 878.9526 - val_loss: 879.9080\n",
      "Epoch 38/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 837.0720 - val_loss: 868.4014\n",
      "Epoch 39/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 812.9901 - val_loss: 858.4233\n",
      "Epoch 40/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 819.3732 - val_loss: 848.0907\n",
      "Epoch 41/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 843.3965 - val_loss: 836.7479\n",
      "Epoch 42/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 833.8607 - val_loss: 826.9427\n",
      "Epoch 43/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 825.4698 - val_loss: 817.8295\n",
      "Epoch 44/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 796.6228 - val_loss: 807.4620\n",
      "Epoch 45/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 793.3768 - val_loss: 797.7554\n",
      "Epoch 46/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 796.2198 - val_loss: 788.7110\n",
      "Epoch 47/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 739.2188 - val_loss: 779.1262\n",
      "Epoch 48/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 752.8442 - val_loss: 769.1468\n",
      "Epoch 49/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 755.4924 - val_loss: 759.1372\n",
      "Epoch 50/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 730.0350 - val_loss: 749.6862\n",
      "Epoch 51/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 738.2347 - val_loss: 739.4087\n",
      "Epoch 52/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 759.1812 - val_loss: 728.9907\n",
      "Epoch 53/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 749.5459 - val_loss: 718.3716\n",
      "Epoch 54/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 709.4243 - val_loss: 707.2277\n",
      "Epoch 55/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 698.8284 - val_loss: 695.6982\n",
      "Epoch 56/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 720.9441 - val_loss: 683.6301\n",
      "Epoch 57/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 672.6161 - val_loss: 671.9632\n",
      "Epoch 58/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 666.6045 - val_loss: 660.4785\n",
      "Epoch 59/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 629.3723 - val_loss: 649.6475\n",
      "Epoch 60/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 658.9512 - val_loss: 638.2750\n",
      "Epoch 61/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 573.5069 - val_loss: 627.0197\n",
      "Epoch 62/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 609.4026 - val_loss: 617.6124\n",
      "Epoch 63/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 648.7368 - val_loss: 604.5862\n",
      "Epoch 64/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 602.0494 - val_loss: 591.5675\n",
      "Epoch 65/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 586.6414 - val_loss: 580.2968\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 553.8740 - val_loss: 569.7731\n",
      "Epoch 67/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 570.9138 - val_loss: 557.0753\n",
      "Epoch 68/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 544.3941 - val_loss: 546.3280\n",
      "Epoch 69/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 547.4971 - val_loss: 534.4062\n",
      "Epoch 70/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 551.0588 - val_loss: 522.6865\n",
      "Epoch 71/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 517.2660 - val_loss: 510.8399\n",
      "Epoch 72/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 527.0154 - val_loss: 497.8427\n",
      "Epoch 73/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 487.2749 - val_loss: 485.9343\n",
      "Epoch 74/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 511.0184 - val_loss: 474.6345\n",
      "Epoch 75/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 464.8322 - val_loss: 463.1515\n",
      "Epoch 76/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 478.6910 - val_loss: 450.8939\n",
      "Epoch 77/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 439.2877 - val_loss: 438.9858\n",
      "Epoch 78/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 444.5269 - val_loss: 427.6514\n",
      "Epoch 79/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 422.6522 - val_loss: 416.9060\n",
      "Epoch 80/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 422.1490 - val_loss: 405.8381\n",
      "Epoch 81/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 383.3707 - val_loss: 394.0514\n",
      "Epoch 82/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 409.5132 - val_loss: 384.2996\n",
      "Epoch 83/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 393.1903 - val_loss: 372.8085\n",
      "Epoch 84/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 372.3655 - val_loss: 361.4261\n",
      "Epoch 85/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 363.4439 - val_loss: 351.1359\n",
      "Epoch 86/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 364.7769 - val_loss: 340.9900\n",
      "Epoch 87/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 325.1625 - val_loss: 331.1962\n",
      "Epoch 88/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 332.2298 - val_loss: 321.5304\n",
      "Epoch 89/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 323.5968 - val_loss: 312.1185\n",
      "Epoch 90/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 302.9207 - val_loss: 301.6595\n",
      "Epoch 91/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 324.1497 - val_loss: 291.5099\n",
      "Epoch 92/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 281.8604 - val_loss: 281.6431\n",
      "Epoch 93/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 272.1657 - val_loss: 271.4517\n",
      "Epoch 94/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 270.9710 - val_loss: 263.2660\n",
      "Epoch 95/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 275.9030 - val_loss: 254.4507\n",
      "Epoch 96/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 243.8046 - val_loss: 244.7471\n",
      "Epoch 97/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 243.2399 - val_loss: 235.6990\n",
      "Epoch 98/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 224.8142 - val_loss: 227.8921\n",
      "Epoch 99/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 210.4398 - val_loss: 221.5004\n",
      "Epoch 100/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 212.0384 - val_loss: 213.1570\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 245.0238 \n",
      "Test Loss: 241.416748046875\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(1) \n",
    "])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(X_train_normalized, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "test_loss = model.evaluate(X_test_normalized, y_test)\n",
    "print(\"Test Loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c90b89",
   "metadata": {},
   "source": [
    " What's the test error for this model? is approximately 241.416748046875"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8056f2f0",
   "metadata": {},
   "source": [
    "Q6/Can you improve the model performance of question 5 by adjusting the number of neurons or the optimization algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4d8cd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 15587.4043 - val_loss: 15547.8691\n",
      "Epoch 2/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15462.7363 - val_loss: 15411.0000\n",
      "Epoch 3/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15355.5908 - val_loss: 15234.9375\n",
      "Epoch 4/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15141.1699 - val_loss: 15022.6240\n",
      "Epoch 5/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14971.4717 - val_loss: 14778.2031\n",
      "Epoch 6/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14644.8564 - val_loss: 14496.9346\n",
      "Epoch 7/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14348.1318 - val_loss: 14173.3613\n",
      "Epoch 8/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14033.4902 - val_loss: 13837.2314\n",
      "Epoch 9/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13634.9629 - val_loss: 13443.4258\n",
      "Epoch 10/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13312.9170 - val_loss: 12999.5996\n",
      "Epoch 11/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12860.7129 - val_loss: 12507.6318\n",
      "Epoch 12/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12254.1387 - val_loss: 11968.1016\n",
      "Epoch 13/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11854.2334 - val_loss: 11430.0977\n",
      "Epoch 14/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11181.6836 - val_loss: 10854.1660\n",
      "Epoch 15/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10622.6680 - val_loss: 10246.9639\n",
      "Epoch 16/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9969.0635 - val_loss: 9592.8838\n",
      "Epoch 17/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9423.5693 - val_loss: 8931.7822\n",
      "Epoch 18/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8727.8350 - val_loss: 8254.0273\n",
      "Epoch 19/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8076.3501 - val_loss: 7559.9780\n",
      "Epoch 20/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7381.0195 - val_loss: 6893.4487\n",
      "Epoch 21/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6679.5049 - val_loss: 6244.0273\n",
      "Epoch 22/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5974.7388 - val_loss: 5598.7168\n",
      "Epoch 23/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5385.6069 - val_loss: 4963.4834\n",
      "Epoch 24/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4697.4551 - val_loss: 4356.4268\n",
      "Epoch 25/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4248.3462 - val_loss: 3812.8867\n",
      "Epoch 26/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3595.6392 - val_loss: 3307.0608\n",
      "Epoch 27/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3069.5764 - val_loss: 2844.6355\n",
      "Epoch 28/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2707.5967 - val_loss: 2427.7654\n",
      "Epoch 29/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2265.5706 - val_loss: 2069.7446\n",
      "Epoch 30/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1947.1622 - val_loss: 1767.7068\n",
      "Epoch 31/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1640.4529 - val_loss: 1519.1366\n",
      "Epoch 32/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1400.8035 - val_loss: 1314.8362\n",
      "Epoch 33/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1273.5621 - val_loss: 1150.8077\n",
      "Epoch 34/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1120.9618 - val_loss: 1032.5162\n",
      "Epoch 35/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1026.1880 - val_loss: 942.8160\n",
      "Epoch 36/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 907.1620 - val_loss: 873.7208\n",
      "Epoch 37/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 897.6283 - val_loss: 820.5978\n",
      "Epoch 38/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 795.8093 - val_loss: 781.5652\n",
      "Epoch 39/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 759.2239 - val_loss: 753.6185\n",
      "Epoch 40/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 732.7918 - val_loss: 734.1149\n",
      "Epoch 41/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 680.7885 - val_loss: 717.0440\n",
      "Epoch 42/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 692.3758 - val_loss: 701.8651\n",
      "Epoch 43/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 662.5264 - val_loss: 689.1360\n",
      "Epoch 44/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 721.7043 - val_loss: 679.4640\n",
      "Epoch 45/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 655.9111 - val_loss: 669.9478\n",
      "Epoch 46/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 676.6188 - val_loss: 661.4031\n",
      "Epoch 47/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 683.4543 - val_loss: 654.1777\n",
      "Epoch 48/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 653.6660 - val_loss: 645.4192\n",
      "Epoch 49/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 647.7086 - val_loss: 637.5437\n",
      "Epoch 50/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 679.5975 - val_loss: 629.9272\n",
      "Epoch 51/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 643.1526 - val_loss: 621.5610\n",
      "Epoch 52/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 656.6652 - val_loss: 613.5022\n",
      "Epoch 53/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 606.9601 - val_loss: 605.3654\n",
      "Epoch 54/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 618.6304 - val_loss: 598.8514\n",
      "Epoch 55/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 605.9403 - val_loss: 592.0746\n",
      "Epoch 56/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 613.5360 - val_loss: 584.8184\n",
      "Epoch 57/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 616.1833 - val_loss: 578.1320\n",
      "Epoch 58/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 572.0356 - val_loss: 571.5494\n",
      "Epoch 59/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 590.1467 - val_loss: 564.9643\n",
      "Epoch 60/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 564.0545 - val_loss: 559.0350\n",
      "Epoch 61/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 516.6852 - val_loss: 553.9149\n",
      "Epoch 62/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 568.1604 - val_loss: 548.2251\n",
      "Epoch 63/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 533.9780 - val_loss: 543.0148\n",
      "Epoch 64/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 538.0684 - val_loss: 539.8734\n",
      "Epoch 65/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 533.7989 - val_loss: 535.9051\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 515.6248 - val_loss: 532.3119\n",
      "Epoch 67/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 531.6867 - val_loss: 528.4968\n",
      "Epoch 68/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 510.6830 - val_loss: 524.9112\n",
      "Epoch 69/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 526.8582 - val_loss: 520.9191\n",
      "Epoch 70/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 515.3533 - val_loss: 516.9612\n",
      "Epoch 71/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 556.2944 - val_loss: 512.8697\n",
      "Epoch 72/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 507.3092 - val_loss: 508.2113\n",
      "Epoch 73/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 517.1626 - val_loss: 503.3065\n",
      "Epoch 74/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 551.3911 - val_loss: 499.2267\n",
      "Epoch 75/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 482.9218 - val_loss: 494.8491\n",
      "Epoch 76/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 476.9716 - val_loss: 490.1405\n",
      "Epoch 77/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 522.5080 - val_loss: 485.5961\n",
      "Epoch 78/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 485.5341 - val_loss: 480.9489\n",
      "Epoch 79/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 503.1314 - val_loss: 475.8721\n",
      "Epoch 80/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 469.1666 - val_loss: 470.4969\n",
      "Epoch 81/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 478.9184 - val_loss: 465.6039\n",
      "Epoch 82/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 463.3441 - val_loss: 460.7740\n",
      "Epoch 83/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 470.5726 - val_loss: 456.2755\n",
      "Epoch 84/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 464.6725 - val_loss: 451.2137\n",
      "Epoch 85/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 463.4368 - val_loss: 446.3526\n",
      "Epoch 86/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 437.0453 - val_loss: 441.3722\n",
      "Epoch 87/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 419.6032 - val_loss: 435.9534\n",
      "Epoch 88/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 406.6660 - val_loss: 430.9505\n",
      "Epoch 89/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 430.1422 - val_loss: 425.5255\n",
      "Epoch 90/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 437.4798 - val_loss: 419.8678\n",
      "Epoch 91/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 445.6366 - val_loss: 416.9666\n",
      "Epoch 92/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 438.1277 - val_loss: 413.7421\n",
      "Epoch 93/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 412.9934 - val_loss: 410.4868\n",
      "Epoch 94/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 421.5818 - val_loss: 407.5184\n",
      "Epoch 95/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 400.5746 - val_loss: 404.2190\n",
      "Epoch 96/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 407.2001 - val_loss: 400.7407\n",
      "Epoch 97/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 404.7410 - val_loss: 398.7883\n",
      "Epoch 98/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 395.0349 - val_loss: 396.2179\n",
      "Epoch 99/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 401.5661 - val_loss: 393.8286\n",
      "Epoch 100/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 419.3006 - val_loss: 391.3022\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 437.6128 \n",
      "Test Loss: 436.2311096191406\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  \n",
    "    tf.keras.layers.Dense(1) \n",
    "])\n",
    "model.compile(optimizer='adamax', loss='mean_squared_error')\n",
    "history = model.fit(X_train_normalized, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "test_loss = model.evaluate(X_test_normalized, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1bb04e",
   "metadata": {},
   "source": [
    "Q7/Build a deep learning regression model to forecast \"Scaled sound pressure level\" using all other features and PyTorch. Please use only two layers of the neuron network. You choose the number of neurons to use in the first layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38b7b46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 15517.9404\n",
      "Epoch 2/20, Loss: 15185.3444\n",
      "Epoch 3/20, Loss: 14622.5822\n",
      "Epoch 4/20, Loss: 13708.5254\n",
      "Epoch 5/20, Loss: 12428.7822\n",
      "Epoch 6/20, Loss: 10853.0105\n",
      "Epoch 7/20, Loss: 9093.6079\n",
      "Epoch 8/20, Loss: 7300.4589\n",
      "Epoch 9/20, Loss: 5611.1962\n",
      "Epoch 10/20, Loss: 4147.7630\n",
      "Epoch 11/20, Loss: 2984.0078\n",
      "Epoch 12/20, Loss: 2143.2135\n",
      "Epoch 13/20, Loss: 1588.1026\n",
      "Epoch 14/20, Loss: 1254.8626\n",
      "Epoch 15/20, Loss: 1072.2051\n",
      "Epoch 16/20, Loss: 978.3416\n",
      "Epoch 17/20, Loss: 927.2351\n",
      "Epoch 18/20, Loss: 895.2332\n",
      "Epoch 19/20, Loss: 870.6987\n",
      "Epoch 20/20, Loss: 847.6873\n",
      "Test Loss: 911.9798\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_neurons = 128\n",
    "X_train_normalized_tensor = torch.tensor(X_train_normalized, dtype=torch.float32)\n",
    "X_test_normalized_tensor = torch.tensor(X_test_normalized, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)  \n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)  \n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, num_neurons):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, num_neurons)\n",
    "        self.fc2 = nn.Linear(num_neurons, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "model = RegressionModel(input_size=X_train_normalized_tensor.shape[1], num_neurons=num_neurons)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train_normalized_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_normalized_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3605fea",
   "metadata": {},
   "source": [
    "What's the test error for this model?is approximately 911.9798"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf0a7af",
   "metadata": {},
   "source": [
    "Q8/Can you improve the model performance of question 7 by adjusting the number of neurons or the optimization algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e08f74dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 15563.2406\n",
      "Epoch 2/20, Loss: 15504.5122\n",
      "Epoch 3/20, Loss: 15423.2436\n",
      "Epoch 4/20, Loss: 15306.9082\n",
      "Epoch 5/20, Loss: 15151.5529\n",
      "Epoch 6/20, Loss: 14960.4816\n",
      "Epoch 7/20, Loss: 14735.3073\n",
      "Epoch 8/20, Loss: 14477.6318\n",
      "Epoch 9/20, Loss: 14189.2355\n",
      "Epoch 10/20, Loss: 13871.9868\n",
      "Epoch 11/20, Loss: 13528.0554\n",
      "Epoch 12/20, Loss: 13158.9913\n",
      "Epoch 13/20, Loss: 12768.7297\n",
      "Epoch 14/20, Loss: 12358.7037\n",
      "Epoch 15/20, Loss: 11931.6346\n",
      "Epoch 16/20, Loss: 11490.2406\n",
      "Epoch 17/20, Loss: 11036.8328\n",
      "Epoch 18/20, Loss: 10574.5693\n",
      "Epoch 19/20, Loss: 10105.7506\n",
      "Epoch 20/20, Loss: 9633.0439\n",
      "Test Loss: 9486.3830\n",
      "Test error for 16 neurons and adam optimizer: 9486.383049340739\n",
      "Epoch 1/20, Loss: 15610.5673\n",
      "Epoch 2/20, Loss: 15563.7920\n",
      "Epoch 3/20, Loss: 15515.5507\n",
      "Epoch 4/20, Loss: 15457.6945\n",
      "Epoch 5/20, Loss: 15386.9189\n",
      "Epoch 6/20, Loss: 15302.4840\n",
      "Epoch 7/20, Loss: 15203.2070\n",
      "Epoch 8/20, Loss: 15091.2490\n",
      "Epoch 9/20, Loss: 14965.7941\n",
      "Epoch 10/20, Loss: 14824.2347\n",
      "Epoch 11/20, Loss: 14670.0905\n",
      "Epoch 12/20, Loss: 14503.3345\n",
      "Epoch 13/20, Loss: 14325.5368\n",
      "Epoch 14/20, Loss: 14134.0892\n",
      "Epoch 15/20, Loss: 13931.2393\n",
      "Epoch 16/20, Loss: 13715.3101\n",
      "Epoch 17/20, Loss: 13486.9916\n",
      "Epoch 18/20, Loss: 13248.9969\n",
      "Epoch 19/20, Loss: 12999.2360\n",
      "Epoch 20/20, Loss: 12738.1726\n",
      "Test Loss: 12620.7073\n",
      "Test error for 16 neurons and adamax optimizer: 12620.707284312708\n",
      "Epoch 1/20, Loss: 4082.2841\n",
      "Epoch 2/20, Loss: 167.7869\n",
      "Epoch 3/20, Loss: 92.4290\n",
      "Epoch 4/20, Loss: 60.2655\n",
      "Epoch 5/20, Loss: 45.6501\n",
      "Epoch 6/20, Loss: 37.2571\n",
      "Epoch 7/20, Loss: 32.9685\n",
      "Epoch 8/20, Loss: 30.1173\n",
      "Epoch 9/20, Loss: 28.2210\n",
      "Epoch 10/20, Loss: 27.1673\n",
      "Epoch 11/20, Loss: 26.1896\n",
      "Epoch 12/20, Loss: 25.4008\n",
      "Epoch 13/20, Loss: 25.0193\n",
      "Epoch 14/20, Loss: 24.8745\n",
      "Epoch 15/20, Loss: 24.6050\n",
      "Epoch 16/20, Loss: 24.1254\n",
      "Epoch 17/20, Loss: 24.3678\n",
      "Epoch 18/20, Loss: 24.1060\n",
      "Epoch 19/20, Loss: 24.0180\n",
      "Epoch 20/20, Loss: 24.2472\n",
      "Test Loss: 22.5224\n",
      "Test error for 16 neurons and sgd optimizer: 22.522422397651546\n",
      "Epoch 1/20, Loss: 15525.4055\n",
      "Epoch 2/20, Loss: 15301.0102\n",
      "Epoch 3/20, Loss: 15087.1702\n",
      "Epoch 4/20, Loss: 14867.9742\n",
      "Epoch 5/20, Loss: 14639.8199\n",
      "Epoch 6/20, Loss: 14400.8412\n",
      "Epoch 7/20, Loss: 14150.7393\n",
      "Epoch 8/20, Loss: 13889.1289\n",
      "Epoch 9/20, Loss: 13617.2057\n",
      "Epoch 10/20, Loss: 13334.9719\n",
      "Epoch 11/20, Loss: 13042.7651\n",
      "Epoch 12/20, Loss: 12740.7557\n",
      "Epoch 13/20, Loss: 12430.0868\n",
      "Epoch 14/20, Loss: 12110.9241\n",
      "Epoch 15/20, Loss: 11784.4530\n",
      "Epoch 16/20, Loss: 11451.1008\n",
      "Epoch 17/20, Loss: 11111.3470\n",
      "Epoch 18/20, Loss: 10766.1757\n",
      "Epoch 19/20, Loss: 10416.0682\n",
      "Epoch 20/20, Loss: 10061.2059\n",
      "Test Loss: 9960.1535\n",
      "Test error for 16 neurons and rmsprop optimizer: 9960.153527953697\n",
      "Epoch 1/20, Loss: 15632.5156\n",
      "Epoch 2/20, Loss: 15527.2171\n",
      "Epoch 3/20, Loss: 15387.0524\n",
      "Epoch 4/20, Loss: 15186.0961\n",
      "Epoch 5/20, Loss: 14916.2499\n",
      "Epoch 6/20, Loss: 14575.6311\n",
      "Epoch 7/20, Loss: 14165.0747\n",
      "Epoch 8/20, Loss: 13683.4934\n",
      "Epoch 9/20, Loss: 13131.3504\n",
      "Epoch 10/20, Loss: 12505.7393\n",
      "Epoch 11/20, Loss: 11806.6848\n",
      "Epoch 12/20, Loss: 11050.5008\n",
      "Epoch 13/20, Loss: 10256.2117\n",
      "Epoch 14/20, Loss: 9429.1196\n",
      "Epoch 15/20, Loss: 8581.3102\n",
      "Epoch 16/20, Loss: 7732.9440\n",
      "Epoch 17/20, Loss: 6900.5525\n",
      "Epoch 18/20, Loss: 6087.2132\n",
      "Epoch 19/20, Loss: 5313.8132\n",
      "Epoch 20/20, Loss: 4598.2246\n",
      "Test Loss: 4417.9066\n",
      "Test error for 32 neurons and adam optimizer: 4417.906590661337\n",
      "Epoch 1/20, Loss: 15589.5985\n",
      "Epoch 2/20, Loss: 15507.5947\n",
      "Epoch 3/20, Loss: 15416.3980\n",
      "Epoch 4/20, Loss: 15308.8943\n",
      "Epoch 5/20, Loss: 15181.0596\n",
      "Epoch 6/20, Loss: 15027.6789\n",
      "Epoch 7/20, Loss: 14848.6722\n",
      "Epoch 8/20, Loss: 14639.6666\n",
      "Epoch 9/20, Loss: 14402.4851\n",
      "Epoch 10/20, Loss: 14141.0024\n",
      "Epoch 11/20, Loss: 13848.7653\n",
      "Epoch 12/20, Loss: 13528.8016\n",
      "Epoch 13/20, Loss: 13182.8279\n",
      "Epoch 14/20, Loss: 12809.8886\n",
      "Epoch 15/20, Loss: 12415.3272\n",
      "Epoch 16/20, Loss: 11995.5789\n",
      "Epoch 17/20, Loss: 11552.9280\n",
      "Epoch 18/20, Loss: 11090.9704\n",
      "Epoch 19/20, Loss: 10607.0422\n",
      "Epoch 20/20, Loss: 10105.9635\n",
      "Test Loss: 9925.9569\n",
      "Test error for 32 neurons and adamax optimizer: 9925.956917696221\n",
      "Epoch 1/20, Loss: 3737.0990\n",
      "Epoch 2/20, Loss: 170.5797\n",
      "Epoch 3/20, Loss: 92.2099\n",
      "Epoch 4/20, Loss: 60.2603\n",
      "Epoch 5/20, Loss: 45.7381\n",
      "Epoch 6/20, Loss: 36.9653\n",
      "Epoch 7/20, Loss: 32.6128\n",
      "Epoch 8/20, Loss: 29.6788\n",
      "Epoch 9/20, Loss: 27.8619\n",
      "Epoch 10/20, Loss: 26.3104\n",
      "Epoch 11/20, Loss: 25.7558\n",
      "Epoch 12/20, Loss: 25.1609\n",
      "Epoch 13/20, Loss: 24.8135\n",
      "Epoch 14/20, Loss: 24.5775\n",
      "Epoch 15/20, Loss: 24.1944\n",
      "Epoch 16/20, Loss: 24.1573\n",
      "Epoch 17/20, Loss: 24.0557\n",
      "Epoch 18/20, Loss: 23.8553\n",
      "Epoch 19/20, Loss: 23.6976\n",
      "Epoch 20/20, Loss: 24.0570\n",
      "Test Loss: 23.9031\n",
      "Test error for 32 neurons and sgd optimizer: 23.903137942089195\n",
      "Epoch 1/20, Loss: 15423.2005\n",
      "Epoch 2/20, Loss: 14985.9803\n",
      "Epoch 3/20, Loss: 14511.3115\n",
      "Epoch 4/20, Loss: 13992.1006\n",
      "Epoch 5/20, Loss: 13439.3640\n",
      "Epoch 6/20, Loss: 12854.6882\n",
      "Epoch 7/20, Loss: 12243.5062\n",
      "Epoch 8/20, Loss: 11613.6606\n",
      "Epoch 9/20, Loss: 10969.8006\n",
      "Epoch 10/20, Loss: 10316.1115\n",
      "Epoch 11/20, Loss: 9656.1633\n",
      "Epoch 12/20, Loss: 8994.4200\n",
      "Epoch 13/20, Loss: 8336.0649\n",
      "Epoch 14/20, Loss: 7684.3851\n",
      "Epoch 15/20, Loss: 7043.4176\n",
      "Epoch 16/20, Loss: 6417.1885\n",
      "Epoch 17/20, Loss: 5809.5419\n",
      "Epoch 18/20, Loss: 5224.8219\n",
      "Epoch 19/20, Loss: 4667.5938\n",
      "Epoch 20/20, Loss: 4141.4342\n",
      "Test Loss: 4062.8617\n",
      "Test error for 32 neurons and rmsprop optimizer: 4062.8616558087624\n",
      "Epoch 1/20, Loss: 15621.8892\n",
      "Epoch 2/20, Loss: 15464.5506\n",
      "Epoch 3/20, Loss: 15233.1277\n",
      "Epoch 4/20, Loss: 14851.9723\n",
      "Epoch 5/20, Loss: 14281.0483\n",
      "Epoch 6/20, Loss: 13504.7466\n",
      "Epoch 7/20, Loss: 12532.5677\n",
      "Epoch 8/20, Loss: 11410.1716\n",
      "Epoch 9/20, Loss: 10187.9319\n",
      "Epoch 10/20, Loss: 8909.4064\n",
      "Epoch 11/20, Loss: 7629.0060\n",
      "Epoch 12/20, Loss: 6401.1750\n",
      "Epoch 13/20, Loss: 5274.5490\n",
      "Epoch 14/20, Loss: 4278.9550\n",
      "Epoch 15/20, Loss: 3427.9307\n",
      "Epoch 16/20, Loss: 2732.9300\n",
      "Epoch 17/20, Loss: 2184.4810\n",
      "Epoch 18/20, Loss: 1770.3843\n",
      "Epoch 19/20, Loss: 1467.2310\n",
      "Epoch 20/20, Loss: 1255.6348\n",
      "Test Loss: 1301.3730\n",
      "Test error for 64 neurons and adam optimizer: 1301.3730249753426\n",
      "Epoch 1/20, Loss: 15552.4777\n",
      "Epoch 2/20, Loss: 15430.1433\n",
      "Epoch 3/20, Loss: 15285.6011\n",
      "Epoch 4/20, Loss: 15106.2020\n",
      "Epoch 5/20, Loss: 14886.5526\n",
      "Epoch 6/20, Loss: 14619.4881\n",
      "Epoch 7/20, Loss: 14304.5027\n",
      "Epoch 8/20, Loss: 13943.4671\n",
      "Epoch 9/20, Loss: 13535.0643\n",
      "Epoch 10/20, Loss: 13085.9409\n",
      "Epoch 11/20, Loss: 12590.5590\n",
      "Epoch 12/20, Loss: 12050.2827\n",
      "Epoch 13/20, Loss: 11470.1572\n",
      "Epoch 14/20, Loss: 10856.2432\n",
      "Epoch 15/20, Loss: 10205.3925\n",
      "Epoch 16/20, Loss: 9528.7793\n",
      "Epoch 17/20, Loss: 8837.6101\n",
      "Epoch 18/20, Loss: 8131.3434\n",
      "Epoch 19/20, Loss: 7415.0732\n",
      "Epoch 20/20, Loss: 6699.1699\n",
      "Test Loss: 6474.6591\n",
      "Test error for 64 neurons and adamax optimizer: 6474.659053156146\n",
      "Epoch 1/20, Loss: 3375.3906\n",
      "Epoch 2/20, Loss: 148.1026\n",
      "Epoch 3/20, Loss: 80.7770\n",
      "Epoch 4/20, Loss: 53.6247\n",
      "Epoch 5/20, Loss: 41.1939\n",
      "Epoch 6/20, Loss: 35.7749\n",
      "Epoch 7/20, Loss: 31.5886\n",
      "Epoch 8/20, Loss: 29.4212\n",
      "Epoch 9/20, Loss: 28.0580\n",
      "Epoch 10/20, Loss: 26.8456\n",
      "Epoch 11/20, Loss: 26.1837\n",
      "Epoch 12/20, Loss: 25.5825\n",
      "Epoch 13/20, Loss: 24.9735\n",
      "Epoch 14/20, Loss: 24.6900\n",
      "Epoch 15/20, Loss: 24.6260\n",
      "Epoch 16/20, Loss: 24.3452\n",
      "Epoch 17/20, Loss: 23.9795\n",
      "Epoch 18/20, Loss: 24.2247\n",
      "Epoch 19/20, Loss: 23.9494\n",
      "Epoch 20/20, Loss: 24.2365\n",
      "Test Loss: 22.8132\n",
      "Test error for 64 neurons and sgd optimizer: 22.813184250232784\n",
      "Epoch 1/20, Loss: 15341.4190\n",
      "Epoch 2/20, Loss: 14684.7744\n",
      "Epoch 3/20, Loss: 13880.3806\n",
      "Epoch 4/20, Loss: 12989.8646\n",
      "Epoch 5/20, Loss: 12055.1165\n",
      "Epoch 6/20, Loss: 11092.4926\n",
      "Epoch 7/20, Loss: 10123.6878\n",
      "Epoch 8/20, Loss: 9162.1576\n",
      "Epoch 9/20, Loss: 8214.9522\n",
      "Epoch 10/20, Loss: 7285.3927\n",
      "Epoch 11/20, Loss: 6390.9426\n",
      "Epoch 12/20, Loss: 5544.6640\n",
      "Epoch 13/20, Loss: 4755.4211\n",
      "Epoch 14/20, Loss: 4031.4717\n",
      "Epoch 15/20, Loss: 3381.1329\n",
      "Epoch 16/20, Loss: 2809.3009\n",
      "Epoch 17/20, Loss: 2320.3510\n",
      "Epoch 18/20, Loss: 1915.8395\n",
      "Epoch 19/20, Loss: 1595.7300\n",
      "Epoch 20/20, Loss: 1352.5839\n",
      "Test Loss: 1384.3696\n",
      "Test error for 64 neurons and rmsprop optimizer: 1384.3696066010434\n",
      "Best hyperparameters:\n",
      "Number of neurons in the first layer: 16\n",
      "Optimizer: sgd\n",
      "Best test error: 22.522422397651546\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_and_evaluate_model(input_size, num_neurons, optimizer_name, epochs):\n",
    "    model = RegressionModel(input_size, num_neurons)\n",
    "    \n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    elif optimizer_name == 'adamax':\n",
    "        optimizer = optim.Adamax(model.parameters(), lr=0.001)\n",
    "    elif optimizer_name == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "    elif optimizer_name == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid optimizer name\")\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_normalized_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "num_neurons_list = [16, 32, 64]\n",
    "optimizers = ['adam', 'adamax', 'sgd', 'rmsprop']\n",
    "epochs = 20\n",
    "\n",
    "best_num_neurons = None\n",
    "best_optimizer = None\n",
    "best_test_error = float('inf')\n",
    "\n",
    "for num_neurons in num_neurons_list:\n",
    "    for optimizer_name in optimizers:\n",
    "        try:\n",
    "            test_error = train_and_evaluate_model(X_train_normalized_tensor.shape[1], num_neurons, optimizer_name, epochs)\n",
    "            print(f\"Test error for {num_neurons} neurons and {optimizer_name} optimizer:\", test_error)\n",
    "        \n",
    "            \n",
    "            if test_error < best_test_error:\n",
    "                best_test_error = test_error\n",
    "                best_num_neurons = num_neurons\n",
    "                best_optimizer = optimizer_name\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "print(\"Best hyperparameters:\")\n",
    "print(\"Number of neurons in the first layer:\", best_num_neurons)\n",
    "print(\"Optimizer:\", best_optimizer)\n",
    "print(\"Best test error:\", best_test_error)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
