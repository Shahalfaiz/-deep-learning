{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab04a27d",
   "metadata": {},
   "source": [
    "## Q1/Download and prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa8a435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import math\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.layers import StringLookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f46f083",
   "metadata": {},
   "outputs": [],
   "source": [
    "urlretrieve(\"http://files.grouplens.org/datasets/movielens/ml-1m.zip\", \"movielens.zip\")\n",
    "ZipFile(\"movielens.zip\", \"r\").extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d824b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "users = pd.read_csv(\n",
    "    \"ml-1m/users.dat\",\n",
    "    sep=\"::\",\n",
    "    names=[\"user_id\", \"sex\", \"age_group\", \"occupation\", \"zip_code\"],\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "ratings = pd.read_csv(\n",
    "    \"ml-1m/ratings.dat\",\n",
    "    sep=\"::\",\n",
    "    names=[\"user_id\", \"movie_id\", \"rating\", \"unix_timestamp\"],\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "movies = pd.read_csv(\n",
    "    \"ml-1m/movies.dat\",\n",
    "    sep=\"::\",\n",
    "    names=[\"movie_id\", \"title\", \"genres\"],\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0107a182",
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"user_id\"] = users[\"user_id\"].apply(lambda x: f\"user_{x}\")\n",
    "users[\"age_group\"] = users[\"age_group\"].apply(lambda x: f\"group_{x}\")\n",
    "users[\"occupation\"] = users[\"occupation\"].apply(lambda x: f\"occupation_{x}\")\n",
    "\n",
    "movies[\"movie_id\"] = movies[\"movie_id\"].apply(lambda x: f\"movie_{x}\")\n",
    "\n",
    "ratings[\"movie_id\"] = ratings[\"movie_id\"].apply(lambda x: f\"movie_{x}\")\n",
    "ratings[\"user_id\"] = ratings[\"user_id\"].apply(lambda x: f\"user_{x}\")\n",
    "ratings[\"rating\"] = ratings[\"rating\"].apply(lambda x: float(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e36bae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = [\"Action\", \"Adventure\", \"Animation\", \"Children's\", \"Comedy\", \"Crime\"]\n",
    "genres += [\"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\", \"Musical\"]\n",
    "genres += [\"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"]\n",
    "\n",
    "for genre in genres:\n",
    "    movies[genre] = movies[\"genres\"].apply(\n",
    "        lambda values: int(genre in values.split(\"|\"))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a94d72",
   "metadata": {},
   "source": [
    "## Q2/Transform the movie ratings data into sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe5f4338",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_group = ratings.sort_values(by=[\"unix_timestamp\"]).groupby(\"user_id\")\n",
    "\n",
    "ratings_data = pd.DataFrame(\n",
    "    data={\n",
    "        \"user_id\": list(ratings_group.groups.keys()),\n",
    "        \"movie_ids\": list(ratings_group.movie_id.apply(list)),\n",
    "        \"ratings\": list(ratings_group.rating.apply(list)),\n",
    "        \"timestamps\": list(ratings_group.unix_timestamp.apply(list)),\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63ecc3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 4\n",
    "step_size = 2\n",
    "\n",
    "def create_sequences(values, window_size, step_size):\n",
    "    sequences = []\n",
    "    start_index = 0\n",
    "    while True:\n",
    "        end_index = start_index + window_size\n",
    "        seq = values[start_index:end_index]\n",
    "        if len(seq) < window_size:\n",
    "            seq = values[-window_size:]\n",
    "            if len(seq) == window_size:\n",
    "                sequences.append(seq)\n",
    "            break\n",
    "        sequences.append(seq)\n",
    "        start_index += step_size\n",
    "    return sequences\n",
    "\n",
    "ratings_data.movie_ids = ratings_data.movie_ids.apply(\n",
    "    lambda ids: create_sequences(ids, sequence_length, step_size)\n",
    ")\n",
    "\n",
    "ratings_data.ratings = ratings_data.ratings.apply(\n",
    "    lambda ids: create_sequences(ids, sequence_length, step_size)\n",
    ")\n",
    "\n",
    "del ratings_data[\"timestamps\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08ff2d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_data_movies = ratings_data[[\"user_id\", \"movie_ids\"]].explode(\"movie_ids\", ignore_index=True)\n",
    "ratings_data_rating = ratings_data[[\"ratings\"]].explode(\"ratings\", ignore_index=True)\n",
    "ratings_data_transformed = pd.concat([ratings_data_movies, ratings_data_rating], axis=1)\n",
    "ratings_data_transformed = ratings_data_transformed.join(users.set_index(\"user_id\"), on=\"user_id\")\n",
    "ratings_data_transformed.movie_ids = ratings_data_transformed.movie_ids.apply(lambda x: \",\".join(x))\n",
    "ratings_data_transformed.ratings = ratings_data_transformed.ratings.apply(lambda x: \",\".join([str(v) for v in x]))\n",
    "\n",
    "del ratings_data_transformed[\"zip_code\"]\n",
    "\n",
    "ratings_data_transformed.rename(columns={\"movie_ids\": \"sequence_movie_ids\", \"ratings\": \"sequence_ratings\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455e7dc2",
   "metadata": {},
   "source": [
    "## Q3/Define metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8daa963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_HEADER = list(ratings_data_transformed.columns)\n",
    "\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \"user_id\": list(users.user_id.unique()),\n",
    "    \"movie_id\": list(movies.movie_id.unique()),\n",
    "    \"sex\": list(users.sex.unique()),\n",
    "    \"age_group\": list(users.age_group.unique()),\n",
    "    \"occupation\": list(users.occupation.unique()),\n",
    "}\n",
    "\n",
    "USER_FEATURES = [\"sex\", \"age_group\", \"occupation\"]\n",
    "\n",
    "MOVIE_FEATURES = [\"genres\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e2aad0",
   "metadata": {},
   "source": [
    "## Q4/Create tf.data.Dataset for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ab013f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n",
    "    def process(features):\n",
    "        movie_ids_string = features[\"sequence_movie_ids\"]\n",
    "        sequence_movie_ids = tf.strings.split(movie_ids_string, \",\").to_tensor()\n",
    "\n",
    "        \n",
    "        features[\"target_movie_id\"] = sequence_movie_ids[:, -1]\n",
    "        features[\"sequence_movie_ids\"] = sequence_movie_ids[:, :-1]\n",
    "\n",
    "        ratings_string = features[\"sequence_ratings\"]\n",
    "        sequence_ratings = tf.strings.to_number(\n",
    "            tf.strings.split(ratings_string, \",\"), tf.dtypes.float32\n",
    "        ).to_tensor()\n",
    "\n",
    "        \n",
    "        target = sequence_ratings[:, -1]\n",
    "        features[\"sequence_ratings\"] = sequence_ratings[:, :-1]\n",
    "\n",
    "        return features, target\n",
    "\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        csv_file_path,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_HEADER,\n",
    "        num_epochs=1,\n",
    "        header=False,\n",
    "        field_delim=\"|\",\n",
    "        shuffle=shuffle,\n",
    "    ).map(process)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e9a10b",
   "metadata": {},
   "source": [
    "## Q5/Create model inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df42988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    return {\n",
    "        \"user_id\": keras.Input(name=\"user_id\", shape=(1,), dtype=\"string\"),\n",
    "        \"sequence_movie_ids\": keras.Input(\n",
    "            name=\"sequence_movie_ids\", shape=(sequence_length - 1,), dtype=\"string\"\n",
    "        ),\n",
    "        \"target_movie_id\": keras.Input(\n",
    "            name=\"target_movie_id\", shape=(1,), dtype=\"string\"\n",
    "        ),\n",
    "        \"sequence_ratings\": keras.Input(\n",
    "            name=\"sequence_ratings\", shape=(sequence_length - 1,), dtype=tf.float32\n",
    "        ),\n",
    "        \"sex\": keras.Input(name=\"sex\", shape=(1,), dtype=\"string\"),\n",
    "        \"age_group\": keras.Input(name=\"age_group\", shape=(1,), dtype=\"string\"),\n",
    "        \"occupation\": keras.Input(name=\"occupation\", shape=(1,), dtype=\"string\"),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70c787a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import StringLookup\n",
    "\n",
    "def encode_input_features(\n",
    "    inputs,\n",
    "    include_user_id=True,\n",
    "    include_user_features=True,\n",
    "    include_movie_features=True,\n",
    "):\n",
    "    encoded_transformer_features = []\n",
    "    encoded_other_features = []\n",
    "\n",
    "    other_feature_names = []\n",
    "    if include_user_id:\n",
    "        other_feature_names.append(\"user_id\")\n",
    "    if include_user_features:\n",
    "        other_feature_names.extend(USER_FEATURES)\n",
    "\n",
    "    \n",
    "    for feature_name in other_feature_names:\n",
    "        vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "        idx = StringLookup(vocabulary=vocabulary, mask_token=None, num_oov_indices=0)(\n",
    "            inputs[feature_name]\n",
    "        )\n",
    "        embedding_dims = int(math.sqrt(len(vocabulary)))\n",
    "        embedding_encoder = layers.Embedding(\n",
    "            input_dim=len(vocabulary),\n",
    "            output_dim=embedding_dims,\n",
    "            name=f\"{feature_name}_embedding\",\n",
    "        )\n",
    "        encoded_other_features.append(embedding_encoder(idx))\n",
    "\n",
    "    if len(encoded_other_features) > 1:\n",
    "        encoded_other_features = layers.concatenate(encoded_other_features)\n",
    "    elif len(encoded_other_features) == 1:\n",
    "        encoded_other_features = encoded_other_features[0]\n",
    "    else:\n",
    "        encoded_other_features = None\n",
    "\n",
    "    \n",
    "    movie_vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[\"movie_id\"]\n",
    "    movie_embedding_dims = int(math.sqrt(len(movie_vocabulary)))\n",
    "    movie_index_lookup = StringLookup(\n",
    "        vocabulary=movie_vocabulary,\n",
    "        mask_token=None,\n",
    "        num_oov_indices=0,\n",
    "        name=\"movie_index_lookup\",\n",
    "    )\n",
    "    movie_embedding_encoder = layers.Embedding(\n",
    "        input_dim=len(movie_vocabulary),\n",
    "        output_dim=movie_embedding_dims,\n",
    "        name=f\"movie_embedding\",\n",
    "    )\n",
    "    genre_vectors = movies[genres].to_numpy()\n",
    "    movie_genres_lookup = layers.Embedding(\n",
    "        input_dim=genre_vectors.shape[0],\n",
    "        output_dim=genre_vectors.shape[1],\n",
    "        embeddings_initializer=keras.initializers.Constant(genre_vectors),\n",
    "        trainable=False,\n",
    "        name=\"genres_vector\",\n",
    "    )\n",
    "    movie_embedding_processor = layers.Dense(\n",
    "        units=movie_embedding_dims,\n",
    "        activation=\"relu\",\n",
    "        name=\"process_movie_embedding_with_genres\",\n",
    "    )\n",
    "\n",
    "    def encode_movie(movie_id):\n",
    "        movie_idx = movie_index_lookup(movie_id)\n",
    "        movie_embedding = movie_embedding_encoder(movie_idx)\n",
    "        encoded_movie = movie_embedding\n",
    "        if include_movie_features:\n",
    "            movie_genres_vector = movie_genres_lookup(movie_idx)\n",
    "            encoded_movie = movie_embedding_processor(\n",
    "                layers.concatenate([movie_embedding, movie_genres_vector])\n",
    "            )\n",
    "        return encoded_movie\n",
    "\n",
    "    target_movie_id = inputs[\"target_movie_id\"]\n",
    "    encoded_target_movie = encode_movie(target_movie_id)\n",
    "\n",
    "    sequence_movies_ids = inputs[\"sequence_movie_ids\"]\n",
    "    encoded_sequence_movies = encode_movie(sequence_movies_ids)\n",
    "\n",
    "    position_embedding_encoder = layers.Embedding(\n",
    "        input_dim=sequence_length,\n",
    "        output_dim=movie_embedding_dims,\n",
    "        name=\"position_embedding\",\n",
    "    )\n",
    "    positions = tf.range(start=0, limit=sequence_length - 1, delta=1)\n",
    "    encodded_positions = position_embedding_encoder(positions)\n",
    "    sequence_ratings = inputs[\"sequence_ratings\"]\n",
    "    sequence_ratings = keras.ops.expand_dims(sequence_ratings, -1)\n",
    "    encoded_sequence_movies_with_poistion_and_rating = layers.Multiply()(\n",
    "        [(encoded_sequence_movies + encodded_positions), sequence_ratings]\n",
    "    )\n",
    "\n",
    "    for i in range(sequence_length - 1):\n",
    "        feature = encoded_sequence_movies_with_poistion_and_rating[:, i, ...]\n",
    "        feature = keras.ops.expand_dims(feature, 1)\n",
    "        encoded_transformer_features.append(feature)\n",
    "    encoded_transformer_features.append(encoded_target_movie)\n",
    "\n",
    "    encoded_transformer_features = layers.concatenate(\n",
    "        encoded_transformer_features, axis=1\n",
    "    )\n",
    "\n",
    "    return encoded_transformer_features, encoded_other_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c3807d",
   "metadata": {},
   "source": [
    "## Q6/Create a BST model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d944925",
   "metadata": {},
   "outputs": [],
   "source": [
    "include_user_id = False\n",
    "include_user_features = False\n",
    "include_movie_features = False\n",
    "\n",
    "hidden_units = [256, 128]\n",
    "dropout_rate = 0.1\n",
    "num_heads = 3\n",
    "\n",
    "def create_model():\n",
    "    inputs = create_model_inputs()\n",
    "    transformer_features, other_features = encode_input_features(\n",
    "        inputs, include_user_id, include_user_features, include_movie_features\n",
    "    )\n",
    "\n",
    "    \n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=transformer_features.shape[2], dropout=dropout_rate\n",
    "    )(transformer_features, transformer_features)\n",
    "\n",
    "    \n",
    "    attention_output = layers.Dropout(dropout_rate)(attention_output)\n",
    "    x1 = layers.Add()([transformer_features, attention_output])\n",
    "    x1 = layers.LayerNormalization()(x1)\n",
    "    x2 = layers.LeakyReLU()(x1)\n",
    "    x2 = layers.Dense(units=x2.shape[-1])(x2)\n",
    "    x2 = layers.Dropout(dropout_rate)(x2)\n",
    "    transformer_features = layers.Add()([x1, x2])\n",
    "    transformer_features = layers.LayerNormalization()(transformer_features)\n",
    "    features = layers.Flatten()(transformer_features)\n",
    "\n",
    "    if other_features is not None:\n",
    "        features = layers.concatenate(\n",
    "            [features, layers.Reshape([other_features.shape[-1]])(other_features)]\n",
    "        )\n",
    "\n",
    "    for num_units in hidden_units:\n",
    "        features = layers.Dense(num_units)(features)\n",
    "        features = layers.BatchNormalization()(features)\n",
    "        features = layers.LeakyReLU()(features)\n",
    "        features = layers.Dropout(dropout_rate)(features)\n",
    "\n",
    "    outputs = layers.Dense(units=1)(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "model = create_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1958d9e",
   "metadata": {},
   "source": [
    "## Q7/Run training and evaluation experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6602af21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1599/1599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 22ms/step - loss: 1.7376 - mean_absolute_error: 1.0175\n",
      "Epoch 2/10\n",
      "\u001b[1m   1/1599\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:35\u001b[0m 135ms/step - loss: 1.2393 - mean_absolute_error: 0.8906"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\aldul\\Lib\\contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1599/1599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 24ms/step - loss: 1.0500 - mean_absolute_error: 0.8185\n",
      "Epoch 3/10\n",
      "\u001b[1m1599/1599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 25ms/step - loss: 0.9769 - mean_absolute_error: 0.7883\n",
      "Epoch 4/10\n",
      "\u001b[1m1599/1599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 26ms/step - loss: 0.9404 - mean_absolute_error: 0.7736\n",
      "Epoch 5/10\n",
      "\u001b[1m1599/1599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 27ms/step - loss: 0.9186 - mean_absolute_error: 0.7639\n",
      "Epoch 6/10\n",
      "\u001b[1m1599/1599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 26ms/step - loss: 0.9027 - mean_absolute_error: 0.7571\n",
      "Epoch 7/10\n",
      "\u001b[1m1599/1599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 27ms/step - loss: 0.8874 - mean_absolute_error: 0.7502\n",
      "Epoch 8/10\n",
      "\u001b[1m1599/1599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 27ms/step - loss: 0.8753 - mean_absolute_error: 0.7456\n",
      "Epoch 9/10\n",
      "\u001b[1m1599/1599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 28ms/step - loss: 0.8694 - mean_absolute_error: 0.7423\n",
      "Epoch 10/10\n",
      "\u001b[1m1599/1599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 28ms/step - loss: 0.8600 - mean_absolute_error: 0.7390\n",
      "Test MAE: 0.76\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adagrad(learning_rate=0.01),\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    metrics=[keras.metrics.MeanAbsoluteError()],\n",
    ")\n",
    "\n",
    "train_dataset = get_dataset_from_csv(\"train_data.csv\", shuffle=True, batch_size=265)\n",
    "\n",
    "model.fit(train_dataset, epochs=10)\n",
    "\n",
    "test_dataset = get_dataset_from_csv(\"test_data.csv\", batch_size=265)\n",
    "\n",
    "_, mae = model.evaluate(test_dataset, verbose=0)\n",
    "print(f\"Test MAE: {round(mae, 3)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
